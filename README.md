# âš½ SofaScore Football Data Scraper

Outil de **web scraping** dÃ©veloppÃ© en **Python** pour extraire automatiquement les statistiques de joueurs de football depuis **SofaScore**. Le scraper parcourt **9 championnats europÃ©ens**, collecte les profils de plus de **2000 joueurs**, extrait leurs statistiques dÃ©taillÃ©es (attributs, valeur marchande, forces/faiblesses, postes) et exporte le tout dans un fichier **Excel** structurÃ©. Le projet intÃ¨gre une architecture modulaire, une gestion robuste des erreurs, et un pipeline CLI complet.

---

## ğŸ“– Ã€ propos du projet

Ce scraper automatise la collecte de donnÃ©es footballistiques Ã  grande Ã©chelle depuis SofaScore, une des plateformes de statistiques sportives les plus complÃ¨tes. Il gÃ¨re les pages dynamiques rendues en JavaScript via **Selenium**, contourne les protections anti-bot avec rotation de **User-Agents** via l'API ScrapeOps, et traite les donnÃ©es brutes Ã  travers un pipeline de nettoyage, formatage et export. Le projet couvre 9 compÃ©titions : Premier League, La Liga, Bundesliga, Serie A, Ligue 1, Liga Portugal, Eredivisie, Champions League et Europa League.

---

## ğŸ—ï¸ Architecture & Structure

```
Python-Data-Scraper/
â”œâ”€â”€ main.py                      â†’ Point d'entrÃ©e CLI (argparse, 6 commandes)
â”œâ”€â”€ config.py                    â†’ Configuration centralisÃ©e (ligues, chemins, clÃ©s API)
â”œâ”€â”€ requirements.txt             â†’ DÃ©pendances Python
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ browser.py               â†’ Configuration Selenium + rotation User-Agent
â”‚   â”œâ”€â”€ player_scraper.py        â†’ Collecte des liens et scraping des stats
â”‚   â””â”€â”€ search_scraper.py        â†’ Re-scraping par recherche de joueurs
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ formatter.py             â†’ Nettoyage des donnÃ©es brutes (regex, normalisation)
â”‚   â”œâ”€â”€ parser.py                â†’ Parsing structurÃ© et export Excel (pandas)
â”‚   â””â”€â”€ deduplicator.py          â†’ DÃ©duplication et dÃ©tection des scrapes manquants
â””â”€â”€ output/                      â†’ Fichiers gÃ©nÃ©rÃ©s (liens, stats, Excel)
```

**8 fichiers Python** | **Architecture modulaire en packages** | **Pipeline CLI complet**

---

## ğŸ”§ CompÃ©tences techniques dÃ©montrÃ©es

### Web Scraping avancÃ© (Selenium)

- **Navigation de pages dynamiques** â€” Selenium WebDriver pour interagir avec le contenu JavaScript rendu cÃ´tÃ© client par SofaScore (Single Page Application React)
- **Waits explicites** â€” `WebDriverWait` + `expected_conditions` (`presence_of_element_located`, `element_to_be_clickable`) pour synchroniser le scraping avec le chargement asynchrone du DOM
- **SÃ©lecteurs CSS multiples** â€” StratÃ©gie de fallback avec plusieurs sÃ©lecteurs CSS par Ã©lÃ©ment (`nationality_selectors`, `club_selectors`, `champ_selectors`) pour gÃ©rer les variations de structure HTML
- **Pagination automatique** â€” Parcours de jusqu'Ã  46 pages par championnat avec `ActionChains.move_to_element().click()` sur les boutons de navigation
- **Gestion des popups** â€” `dismiss_cookie_popup()` pour fermer automatiquement les banniÃ¨res de consentement
- **Gestion d'exceptions robuste** â€” `try/except` sur `TimeoutException`, `NoSuchElementException`, `StaleElementReferenceException` pour un scraping rÃ©silient sans crash

### Anti-dÃ©tection & Contournement

- **Rotation de User-Agents** â€” IntÃ©gration de l'API ScrapeOps (`headers.scrapeops.io`) pour rÃ©cupÃ©rer une liste de User-Agents rÃ©alistes et en sÃ©lectionner un alÃ©atoirement Ã  chaque session
- **Configuration du driver** â€” `page_load_strategy = "none"` pour ne pas attendre le chargement complet, `--ignore-certificate-errors`, `--allow-running-insecure-content` pour Ã©viter les blocages
- **Variables d'environnement** â€” ClÃ© API stockÃ©e dans `.env` via `python-dotenv`, jamais en dur dans le code

### Architecture logicielle & Design Patterns

- **Architecture modulaire** â€” SÃ©paration en 2 packages (`scraper/`, `processing/`) avec responsabilitÃ©s distinctes : collecte, nettoyage, parsing, export
- **Configuration centralisÃ©e** â€” `config.py` regroupe les URLs des 9 ligues, les chemins de sortie, les paramÃ¨tres du navigateur, facilitant la maintenance et l'extension
- **Pattern Command** â€” Interface CLI avec `argparse` et sous-commandes (`collect-links`, `scrape-stats`, `rescrape`, `format`, `export`, `pipeline`), dispatch via dictionnaire de fonctions
- **Pipeline composable** â€” La commande `pipeline` chaÃ®ne les 4 Ã©tapes dans l'ordre : collecte â†’ scraping â†’ formatage â†’ export, chaque Ã©tape pouvant Ãªtre exÃ©cutÃ©e indÃ©pendamment
- **Logging structurÃ©** â€” Module `logging` avec format horodatÃ© et niveaux (INFO, WARNING, DEBUG) dans chaque module

### Traitement de donnÃ©es (Processing)

- **Nettoyage regex** â€” `format_stats_file()` supprime les unitÃ©s (`yrs`, `cm`), normalise les espaces, et insÃ¨re des dÃ©limiteurs pipe (`|`) entre mots CamelCase via `re.sub(r"([a-z])([A-Z])", r"\1|\2")`
- **Parsing structurÃ©** â€” `parse_stats_file()` tokenize chaque ligne et reconstruit les champs (nom, nationalitÃ©, club, postes, stats) avec une logique de dÃ©tection positionnelle (codes de 1-2 lettres majuscules)
- **DÃ©duplication multi-critÃ¨res** â€” `remove_duplicates()` utilise un `set` de tuples `(Prenom, Nom, Age, Taille, Club)` pour Ã©liminer les doublons
- **DÃ©tection des scrapes manquants** â€” `find_missing_scrapes()` compare les slugs des liens collectÃ©s avec ceux dÃ©jÃ  scrapÃ©s pour identifier les joueurs Ã  re-scraper
- **Export Excel** â€” Conversion en `DataFrame` pandas et export via `openpyxl` vers `.xlsx`

### Extraction de donnÃ©es spÃ©cialisÃ©e

- **Profils joueurs complets** â€” 15+ champs extraits par joueur : nom, nationalitÃ©, championnat, club, postes, Ã¢ge, taille, 5 attributs de performance, valeur marchande, forces, faiblesses, pied prÃ©fÃ©rÃ©
- **Distinction gardien/joueur de champ** â€” Logique conditionnelle avec `attr_map` diffÃ©rent selon la position (`GK` â†’ saves, anticipation, ball_distribution vs. `FW/MF/DF` â†’ attacking, creativity, defending)
- **Extraction de la valeur marchande** â€” Filtrage de caractÃ¨res avec `set("0123456789MKâ‚¬.")` pour nettoyer les valeurs brutes (`â‚¬12.5M` â†’ `125Mâ‚¬`)
- **Forces et faiblesses** â€” Extraction des spans HTML dans les containers SofaScore avec concatÃ©nation par pipe

### Re-scraping intelligent

- **Recherche par nom** â€” `search_scraper.py` utilise la barre de recherche SofaScore pour retrouver les joueurs manquants par leur slug converti en nom (`david-raya` â†’ `david raya`)
- **DÃ©tection automatique** â€” `find_missing_scrapes()` identifie automatiquement les joueurs qui n'ont pas Ã©tÃ© scrapÃ©s avec succÃ¨s
- **Reprise sur erreur** â€” Option `--start` pour reprendre le scraping Ã  un index donnÃ© aprÃ¨s une interruption

---

## â–¶ï¸ Installation & Lancement

### PrÃ©requis
- **Python** â‰¥ 3.8
- **Google Chrome** installÃ©
- **ChromeDriver** (gÃ©rÃ© automatiquement par `webdriver-manager`)

### Installation
```bash
git clone https://github.com/NelsonCamara/Python-Data-Scraper.git
cd Python-Data-Scraper
pip install -r requirements.txt
```

### Configuration
CrÃ©er un fichier `.env` Ã  la racine du projet :
```env
SCRAPEOPS_API_KEY=votre_cle_api
```

### Commandes disponibles

| Commande | Description |
|----------|-------------|
| `python main.py collect-links` | Collecter les URLs des joueurs depuis les pages de championnats |
| `python main.py scrape-stats` | Scraper les statistiques de chaque joueur |
| `python main.py rescrape` | Re-scraper les joueurs manquants ou Ã©chouÃ©s |
| `python main.py format` | Nettoyer et formater le fichier de stats brut |
| `python main.py export` | Parser les stats et exporter en Excel |
| `python main.py pipeline` | ExÃ©cuter le pipeline complet (4 Ã©tapes) |

### Options
```bash
python main.py --headless scrape-stats --start 150   # Mode sans GUI, reprendre Ã  l'index 150
python main.py rescrape --names david-raya nick-pope  # Re-scraper des joueurs spÃ©cifiques
python main.py format --input raw.txt --output clean.txt  # Fichiers personnalisÃ©s
```

---

## ğŸ“Š DonnÃ©es extraites

### Joueur de champ
| Champ | Exemple |
|-------|---------|
| Nom, PrÃ©nom | MbappÃ©, Kylian |
| NationalitÃ© | FRA |
| Championnat | La Liga |
| Club | Real Madrid |
| Postes | FW ST RW |
| Ã‚ge | 26 |
| Taille | 178 |
| Attaque / CrÃ©ativitÃ© / Technique / DÃ©fense / Tactique | 92 / 85 / 88 / 34 / 76 |
| Valeur marchande | 180Mâ‚¬ |
| Forces / Faiblesses | Key passes\|Dribbles / Aerial duels |
| Pied prÃ©fÃ©rÃ© | Right |

### Gardien
| Champ | Exemple |
|-------|---------|
| Saves / Anticipation / Tactique / Distribution / Jeu aÃ©rien | 88 / 82 / 75 / 70 / 85 |

---

## ğŸ› ï¸ Technologies

| Technologie | Usage |
|-------------|-------|
| Python 3 | Langage principal |
| Selenium 4 | Automatisation du navigateur, scraping de pages dynamiques |
| pandas | Manipulation de donnÃ©es et export Excel |
| openpyxl | Ã‰criture de fichiers `.xlsx` |
| requests | Appels API ScrapeOps pour les User-Agents |
| argparse | Interface en ligne de commande |
| python-dotenv | Gestion des variables d'environnement |
| regex (re) | Nettoyage et formatage des donnÃ©es |
| logging | Journalisation structurÃ©e |

---

## ğŸ“„ Championnats couverts

Premier League (46 pages) Â· La Liga (35) Â· Bundesliga (37) Â· Serie A (28) Â· Ligue 1 (30) Â· Liga Portugal (26) Â· Eredivisie (26) Â· Champions League (29) Â· Europa League (30)

---

## ğŸ‘¤ Auteur

**Nelson Camara** â€” Ã‰tudiant en Master Informatique

---

*Projet personnel â€” Scraper de donnÃ©es footballistiques Ã  grande Ã©chelle avec pipeline de traitement automatisÃ©.*
